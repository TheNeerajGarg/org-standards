# Introspection: [Brief Task Description]

**Date**: YYYY-MM-DD
**Type**: [smooth | challenges | mistakes | learning]
**Focus**: Meta-learning for system improvement

---

## Purpose

**This is NOT a task log. This is meta-learning.**

Goal: Identify how this task could have been done more **efficiently and effectively**.

Focus on:
- What slowed us down?
- What could have prevented that?
- How do we improve the system (human + bot + tooling)?

---

## Challenges Encountered

**If smooth**: "None - straightforward implementation"

**If challenges**: Document friction and time wasted
- What blocked progress?
- How much time wasted?
- What caused the friction?

---

## Mistakes Made

**If none**: "None - correct approach used"

**If mistakes**: Document errors and root causes
- What was done wrong?
- Why did it happen? (habit? assumption? unclear docs?)
- What should have been done instead?

---

## How to Avoid Next Time

### What Human Could Have Done Differently
**Focus: Task specification, context, timing**

Questions to guide learning:
- Was the task specification clear enough?
- Was sufficient context provided upfront?
- Were constraints/requirements explicit?
- Should human have asked different questions first?
- Was this the right time to work on this?

**If no improvement needed**: "Task spec was clear and complete"

### What Bot Could Have Done Differently
**Focus: Approach, questions, thoroughness**

Questions to guide learning:
- Should bot have asked clarifying questions upfront?
- Was the approach efficient (or over-engineered/under-engineered)?
- Should bot have read different context first?
- Did bot follow the right patterns from CLAUDE.md?
- Was bot too proactive or too conservative?
- Did bot verify assumptions before proceeding?

**If no improvement needed**: "Approach was efficient and correct"

### What System Could Have Done Differently
**Focus: CLAUDE.md, tooling, workflows, documentation**

Questions to guide learning:
- What rule/pattern should be added to CLAUDE.md?
- What anti-pattern should be documented?
- What automation/script would have helped?
- What validation should exist (hook, CI, etc.)?
- What documentation was missing or unclear?
- What workflow could be streamlined?

**If no improvement needed**: "System/process worked well"

---

## Examples

### Example 1: Smooth Task
```markdown
## Challenges Encountered
None - straightforward implementation

## Mistakes Made
None - correct approach used

## How to Avoid Next Time

### What Human Could Have Done Differently
N/A - task spec was clear

### What Bot Could Have Done Differently
N/A - efficient approach

### What System Could Have Done Differently
Process worked well. No improvements needed.
```

### Example 2: Learning Opportunity
```markdown
## Challenges Encountered
- Spent 15 min figuring out org-standards propagation pattern
- Unclear where shared scripts should live

## Mistakes Made
- Created script in Syra (repo-specific)
- Should have checked org-standards structure first

## How to Avoid Next Time

### What Human Could Have Done Differently
- Ask: "Should this be in org-standards?" upfront
- Provide clearer context about multi-repo impact

### What Bot Could Have Done Differently
- Read org-standards structure BEFORE implementing
- Ask: "Is this shared across repos?" before choosing location
- Pattern: If multi-repo → check org-standards first

### What System Could Have Done Differently
- CLAUDE.md: Add rule "Shared tooling → org-standards"
- Document org-standards propagation pattern
- Create decision tree: repo-specific vs shared
```

---

## Required Sections (for validation)

Pre-commit hook checks these sections exist:
- ✅ Challenges Encountered
- ✅ Mistakes Made
- ✅ How to Avoid Next Time

Content can be "None" or "N/A" for smooth tasks.
